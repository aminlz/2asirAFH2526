<!DOCTYPE html>
<html lang="es">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Cuantización IA - Phi-3 Mini</title>

<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800&display=swap" rel="stylesheet">

<style>

body{
    font-family:'Inter',sans-serif;
    background:#f1f5f9;
    margin:0;
    color:#1e293b;
    line-height:1.75;
}

/* HERO */
.hero{
    background:linear-gradient(135deg,#667eea,#764ba2);
    color:white;
    text-align:center;
    padding:80px 20px;
}

.hero h1{
    font-size:clamp(2.4rem,6vw,4rem);
    margin-bottom:10px;
}

/* CONTENEDOR */
.container{
    max-width:1000px;
    margin:auto;
    padding:40px 20px;
}

/* SECCIONES */
.section{
    background:white;
    padding:35px;
    border-radius:20px;
    margin-bottom:40px;
    box-shadow:0 18px 40px rgba(0,0,0,.12);
}

.section h2{
    margin-bottom:18px;
    font-size:1.5rem;
    color:#4f46e5;
}

/* IMÁGENES */
.image-wrapper{
    text-align:center;
    margin-bottom:20px;
}

.image-wrapper img{
    max-width:100%;
    height:auto;
    border-radius:12px;
    box-shadow:0 10px 30px rgba(0,0,0,.2);
}

/* PROPIEDADES WINDOWS (MUY GRANDES) */
.image-properties img{
    max-height:420px;
    width:auto;
    object-fit:contain;
}

/* TEXTO */
.section p{
    font-size:1.08rem;
    color:#475569;
    margin-bottom:14px;
}

strong{color:#0f172a;}

code{
    background:#eef2ff;
    padding:3px 8px;
    border-radius:6px;
}

/* CONCLUSIÓN */
.conclusion{
    background:linear-gradient(135deg,#10b981,#059669);
    color:white;
    text-align:center;
    padding:70px 30px;
}

.conclusion h2{
    font-size:2rem;
    margin-bottom:20px;
}

</style>
</head>

<body>

<!-- HERO -->
<section class="hero">
<h1>⚡ Cuantización IA — Phi-3 Mini</h1>
<p>Benchmark experimental en LM Studio · RTX 5060 Ti</p>
</section>


<div class="container">

<!-- CAPTURA 1 -->
<div class="section">
<h2>Captura 1 · Phi-3 Q8 Cuantizado</h2>

<div class="image-wrapper">
<img src="cu1.png">
</div>

<p>
La primera captura muestra la inferencia del modelo
<strong>phi-3-mini-q8.gguf</strong> respondiendo
“¡Hola! ¿En qué puedo ayudarte hoy?” con una velocidad de
<strong>64.59 tokens/segundo</strong>, generando 17 tokens en apenas
<strong>0.02 segundos hasta el primer token</strong>.
</p>

<p>
Este rendimiento es excelente para un modelo cuantizado Q8 en GPU,
confirmando que la compresión mantiene coherencia en las respuestas.
La mayor velocidad demuestra que la cuantización optimiza la inferencia
sin pérdida perceptible de calidad en prompts simples.
</p>
</div>


<!-- CAPTURA 2 -->
<div class="section">
<h2>Captura 2 · Propiedades Modelo Q8</h2>

<div class="image-wrapper image-properties">
<img src="cu2.png">
</div>

<p>
La segunda captura muestra las propiedades del archivo
<strong>phi-3-mini-q8.gguf</strong> en el Explorador de Windows,
ocupando <strong>3.78 GB</strong> en disco.
</p>

<p>
Ubicación:
<code>C:\Users\aulateca26\.lmstudio\models\Imstudio</code>.
El archivo fue creado el 23 de enero de 2026 con atributos
solo lectura y oculto, comportamiento normal en modelos GGUF
gestionados por LM Studio.
</p>

<p>
La reducción aproximada del <strong>47% respecto al FP16</strong>
valida el objetivo principal de la cuantización:
mejor eficiencia de almacenamiento.
</p>
</div>


<!-- CAPTURA 3 -->
<div class="section">
<h2>Captura 3 · Modelo Original FP16</h2>

<div class="image-wrapper">
<img src="cu3.png">
</div>

<p>
La tercera captura ejecuta el modelo original
<strong>phi-3-mini-4k-instruct-fp16.gguf (7.11 GB)</strong>,
generando una respuesta más elaborada a
<strong>52.11 tokens/segundo</strong>.
</p>

<p>
El modelo FP16 resulta aproximadamente un
<strong>20% más lento</strong> que Q8,
pero produce respuestas más extensas y detalladas.
Esta comparación demuestra el trade-off clásico:
Q8 prioriza velocidad y eficiencia,
mientras FP16 mantiene máxima precisión numérica.
</p>
</div>


<!-- CAPTURA 4 -->
<div class="section">
<h2>Captura 4 · Propiedades Modelo FP16</h2>

<div class="image-wrapper image-properties">
<img src="cu4.png">
</div>

<p>
Las propiedades del archivo
<strong>phi-3-mini-4k-instruct-fp16.gguf</strong>
confirman un tamaño de <strong>7.11 GB</strong>,
ubicado en:
<code>C:\Users\aulateca26\.lmstudio\models\microsc</code>.
</p>

<p>
La diferencia de rutas indica descargas independientes para
cada formato dentro de LM Studio.
El tamaño prácticamente duplicado frente a Q8 confirma
experimentalmente el éxito del proceso de cuantización.
</p>
</div>

</div>


<!-- CONCLUSIÓN -->
<section class="conclusion">
<h2>Conclusión Experimental</h2>

<p style="font-size:1.25rem;max-width:800px;margin:auto;">
✅ 47% menos tamaño<br>
✅ ~24% más velocidad<br>
✅ Trade-off validado entre eficiencia y precisión<br>
Modelo optimizado para inferencia local en GPU doméstica.
</p>
</section>

</body>
</html>
